{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "from lxml import etree\n",
    "from sys import getsizeof\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "NAMESPACE = '{http://www.mediawiki.org/xml/export-0.10/}'\n",
    "ARTICLE_ID = '0'\n",
    "NAMESPACE_TAG = NAMESPACE + 'ns'\n",
    "PAGE_TAG = NAMESPACE + 'page'\n",
    "TITLE_TAG = NAMESPACE + 'title'\n",
    "REDIRECT_TAG = NAMESPACE + 'redirect'\n",
    "TEXT_TAG = NAMESPACE + 'text'\n",
    "SOURCE = './wiki-dump/enwiki-1gb'\n",
    "DEST = './wiki-dump/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Utils:\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iteration over xml with low ram consumption\n",
    "def fast_iter(context, func, dest):\n",
    "    for event, elem in context:\n",
    "        # Execute operations over xml node\n",
    "        stop = func(elem, dest)\n",
    "        \n",
    "        # Clear data read\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "        \n",
    "        if stop:\n",
    "            break\n",
    "    \n",
    "    del context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------------\n",
    "Split wikipedia dump xml:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "xml_header = '<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\\n'\n",
    "xml_tail = '</mediawiki>\\n'\n",
    "\n",
    "n = 2\n",
    "name = './wiki-dump/enwiki-part'\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_header)\n",
    "\n",
    "# Splits wikipedia dump into n parts choosing randomly the articles of each part\n",
    "for event, elem in context:\n",
    "    # Execute operations over xml node\n",
    "    temp_prob = 0\n",
    "    rand = random()\n",
    "    for i in range(1, n + 1):\n",
    "        if temp_prob <= rand < i / n:\n",
    "            filename = name + str(i)\n",
    "            with open(filename, 'ab') as f:\n",
    "                wiki_page = etree.tostring(elem)\n",
    "                f.write(wiki_page)\n",
    "            break\n",
    "        else:\n",
    "            temp_prob = i / n\n",
    "    \n",
    "    # Clear data read\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_tail)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "Extract keyphrases:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 204163 ngrams --- 96.00009155273438 MB\n",
      "--- 89.53760933876038 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# clean_pattern = '\\[\\[(image:|file:|category:)(.+)\\]\\]'\n",
    "# clean_pattern2 = '\\[\\[(image:|file:|category:)(.*(<br>\\n)*)*]]'\n",
    "extract_pattern = '\\[\\[([^][]+)\\]\\]'\n",
    "\n",
    "ignore = ['Image:', 'File:', 'Category:', 'Q:', 'image:', 'file:', 'category:', 'q:']\n",
    "          #, '<span style=', 'talk:', 'Talk:', '<font style=']\n",
    "\n",
    "# clean_regex = re.compile(clean_pattern, re.IGNORECASE)\n",
    "extract_regex = re.compile(extract_pattern, re.IGNORECASE)\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/mini/enwiki-test1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "vocabulary = Counter()\n",
    "for event, elem in context:\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = iterator.__next__().text\n",
    "    \n",
    "    # Extract data only over articles, exclude: talks, mediawiki, portal, user, etc\n",
    "    if namespace_id == ARTICLE_ID:\n",
    "        new_keys = []\n",
    "        # Extract titles of pages\n",
    "        try:\n",
    "            iterator = elem.iterchildren(tag=REDIRECT_TAG)\n",
    "            redirect_child = iterator.__next__()\n",
    "            page_title = redirect_child.attrib['title'].lower()\n",
    "\n",
    "        except StopIteration:\n",
    "            iterator = elem.iterchildren(tag=TITLE_TAG)\n",
    "            title_child = iterator.__next__()\n",
    "            page_title = title_child.text.lower()\n",
    "\n",
    "        new_keys.append(page_title)\n",
    "\n",
    "        # Extract keyphrases in other morphological form\n",
    "        iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "        text = iterator.__next__().text\n",
    "        keyphrases = extract_regex.findall(text)\n",
    "        for key in keyphrases:\n",
    "            if not any(x in key for x in ignore) and len(key) > 0:\n",
    "                new_keys.append(key.split('|')[-1].lower())\n",
    "\n",
    "        vocabulary.update(new_keys)\n",
    "\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "# Delete keyphrases that occurs less than 5 times\n",
    "for key, value in list(vocabulary.items()):\n",
    "    if value < 5:\n",
    "        del vocabulary[key]\n",
    "\n",
    "\n",
    "print('Vocabulary: {} ngrams --- {} MB'.format(len(vocabulary), getsizeof(vocabulary) / 1024**2))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "# Save vocabulary\n",
    "with open('./wiki-dump/vocabulary_dict', 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)\n",
    "\n",
    "# Vocabulary: 204163 ngrams - 97.37 sec - enwiki-test1gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 18, 16, 16, 15, 15, 15, 15, 14, 14, 14, 14, 13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "top_10 = nlargest(100, vocabulary.keys(), key=lambda x: len(x.split()))\n",
    "# print(top_100)\n",
    "print([len(i.split()) for i in top_10])\n",
    "# print(sum([len(i.split()) for i in vocabulary.keys()]) / len(vocabulary))  # ==> 2.09\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------------------\n",
    "Count N-grams in articles:\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with open('./wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "vec = CountVectorizer(lowercase=True, ngram_range=(1, 21), vocabulary=vocabulary.keys(), token_pattern='[\\S]+')\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/mini/enwiki-test1gb', tag=PAGE_TAG , events=('end',))\n",
    "articles = []\n",
    "keys_count = np.zeros((1, len(vocabulary.keys())))\n",
    "\n",
    "for i, (event, elem) in enumerate(context):\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = iterator.__next__().text\n",
    "\n",
    "    if namespace_id == ARTICLE_ID:\n",
    "        if i % 10000 == 0:\n",
    "            m = vec.transform(articles)\n",
    "            m = m.sum(axis=0)\n",
    "            keys_count = np.add(keys_count, m)\n",
    "            articles = []\n",
    "        else:\n",
    "            iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "            text = iterator.__next__().text\n",
    "            articles.append(text)\n",
    "\n",
    "    # Clear data read\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "\n",
    "# Leave this or not?\n",
    "for index, keyword in enumerate(vec.get_feature_names()):\n",
    "    keys_count[0, index] += vocabulary[keyword]\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save data\n",
    "with open('wiki-dump/keys_count', 'wb') as f:\n",
    "    pickle.dump(keys_count, f)\n",
    "\n",
    "# 114.47 sec, ngram=(1, 20) 1,000\n",
    "# 86.39 sec, ngram=(1, 16) 1,000\n",
    "# 47.87 sec, ngram=(1, 10) 1,000\n",
    "# 406.82 sec, ngram=(1, 16) 5,000\n",
    "# 843.35 sec, ngram=(1, 16) 10,000\n",
    "# 3443.72 sec, ngram=(1, 16) iterando de a 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ft[-2])\n",
    "print(vec.vocabulary_['caught in the act'])\n",
    "print(keys_count[0, 1561442])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "Ranking method, Keyphraseness:\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyphraseness = np.zeros((1, len(vocabulary.keys())))\n",
    "\n",
    "for index, keyword in enumerate(vec.get_feature_names()):\n",
    "    keyphraseness[0, index] = vocabulary[keyword] / keys_count[0, index]\n",
    "\n",
    "# Save data\n",
    "with open('wiki-dump/keyphraseness', 'wb') as f:\n",
    "    pickle.dump(keyphraseness, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyphraseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
