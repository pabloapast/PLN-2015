{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "from lxml import etree\n",
    "from sys import getsizeof\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "NAMESPACE = '{http://www.mediawiki.org/xml/export-0.10/}'\n",
    "ARTICLE_ID = '0'\n",
    "NAMESPACE_TAG = NAMESPACE + 'ns'\n",
    "PAGE_TAG = NAMESPACE + 'page'\n",
    "TITLE_TAG = NAMESPACE + 'title'\n",
    "REDIRECT_TAG = NAMESPACE + 'redirect'\n",
    "TEXT_TAG = NAMESPACE + 'text'\n",
    "SOURCE = './wiki-dump/enwiki-1gb'\n",
    "DEST = './wiki-dump/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Utils:\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iteration over xml with low ram consumption\n",
    "def fast_iter(context, func, dest):\n",
    "    for event, elem in context:\n",
    "        # Execute operations over xml node\n",
    "        stop = func(elem, dest)\n",
    "        \n",
    "        # Clear data read\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "        \n",
    "        if stop:\n",
    "            break\n",
    "    \n",
    "    del context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------------\n",
    "Split wikipedia dump xml:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "xml_header = '<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\\n'\n",
    "xml_tail = '</mediawiki>\\n'\n",
    "\n",
    "n = 2\n",
    "name = './wiki-dump/enwiki-part'\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_header)\n",
    "\n",
    "# Splits wikipedia dump into n parts choosing randomly the articles of each part\n",
    "for event, elem in context:\n",
    "    # Execute operations over xml node\n",
    "    temp_prob = 0\n",
    "    rand = random()\n",
    "    for i in range(1, n + 1):\n",
    "        if temp_prob <= rand < i / n:\n",
    "            filename = name + str(i)\n",
    "            with open(filename, 'ab') as f:\n",
    "                wiki_page = etree.tostring(elem)\n",
    "                f.write(wiki_page)\n",
    "            break\n",
    "        else:\n",
    "            temp_prob = i / n\n",
    "    \n",
    "    # Clear data read\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_tail)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "Extract keyphrases:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean_pattern = '\\[\\[(image:|file:|category:)(.+)\\]\\]'\n",
    "# clean_pattern2 = '\\[\\[(image:|file:|category:)(.*(<br>\\n)*)*]]'\n",
    "extract_pattern = '\\[\\[([^][]+)\\]\\]'\n",
    "\n",
    "ignore = ['Image:', 'File:', 'Category:', 'Q:', 'Wikipedia:', 'image:', 'file:', 'category:', 'q:', 'wikipedia:']\n",
    "          #, '<span style=', 'talk:', 'Talk:', '<font style=']\n",
    "\n",
    "# clean_regex = re.compile(clean_pattern, re.IGNORECASE)\n",
    "extract_regex = re.compile(extract_pattern, re.IGNORECASE)\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/mini/enwiki-test1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "vocabulary = Counter()\n",
    "for event, elem in context:\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = iterator.__next__().text\n",
    "    \n",
    "    # Extract data only over articles, exclude: talks, mediawiki, portal, user, etc\n",
    "    if namespace_id == ARTICLE_ID:\n",
    "        new_keys = []\n",
    "        # Extract titles of pages\n",
    "        try:\n",
    "            iterator = elem.iterchildren(tag=REDIRECT_TAG)\n",
    "            redirect_child = iterator.__next__()\n",
    "            page_title = redirect_child.attrib['title'].lower()\n",
    "\n",
    "        except StopIteration:\n",
    "            iterator = elem.iterchildren(tag=TITLE_TAG)\n",
    "            title_child = iterator.__next__()\n",
    "            page_title = title_child.text.lower()\n",
    "\n",
    "        new_keys.append(page_title)\n",
    "\n",
    "        # Extract keyphrases in other morphological form\n",
    "        iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "        text = iterator.__next__().text\n",
    "        keyphrases = extract_regex.findall(text)\n",
    "        for key in keyphrases:\n",
    "            if not any(x in key for x in ignore) and len(key) > 0:\n",
    "                new_keys.append(key.split('|')[-1].lower())\n",
    "\n",
    "        vocabulary.update(new_keys)\n",
    "\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "# Delete keyphrases that occurs less than 5 times\n",
    "for key, value in list(vocabulary.items()):\n",
    "    if value <= 5:\n",
    "        del vocabulary[key]\n",
    "\n",
    "\n",
    "print('Vocabulary: {} ngrams --- {} MB'.format(len(vocabulary), getsizeof(vocabulary) / 1024**2))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "# Save vocabulary\n",
    "with open('./wiki-dump/vocabulary_dict', 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)\n",
    "\n",
    "# Vocabulary: 204163 ngrams - 97.37 sec - enwiki-test1gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_10 = nlargest(100, vocabulary.keys(), key=lambda x: len(x.split()))\n",
    "# print(top_100)\n",
    "print([len(i.split()) for i in top_10])\n",
    "# print(sum([len(i.split()) for i in vocabulary.keys()]) / len(vocabulary))  # ==> 2.09\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------------------\n",
    "Count N-grams in articles:\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with open('./wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 16), vocabulary=vocabulary.keys(), token_pattern='[\\S]+')\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/mini/enwiki-test1gb', tag=PAGE_TAG , events=('end',))\n",
    "articles = []\n",
    "keys_count = np.zeros((1, len(vocabulary.keys())))\n",
    "\n",
    "for i, (event, elem) in enumerate(context):\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = iterator.__next__().text\n",
    "\n",
    "    if namespace_id == ARTICLE_ID:\n",
    "        if i % 10000 == 0:\n",
    "            m = vectorizer.transform(articles)\n",
    "            m = m.sum(axis=0)\n",
    "            keys_count = np.add(keys_count, m)\n",
    "            articles = []\n",
    "        else:\n",
    "            iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "            text = iterator.__next__().text\n",
    "            articles.append(text)\n",
    "\n",
    "    # Clear data read\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "\n",
    "# Leave this or not?\n",
    "# I take advantage of this loop to build a dict from vocabulary keys to their index in the array\n",
    "key_to_index = dict()\n",
    "for index, keyword in enumerate(vectorizer.get_feature_names()):\n",
    "    keys_count[0, index] += vocabulary[keyword]\n",
    "    key_to_index[keyword] = index\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save data\n",
    "# with open('wiki-dump/vectorizer', 'wb') as f:\n",
    "#     pickle.dump(vectorizer, f)\n",
    "\n",
    "with open('wiki-dump/keys_count', 'wb') as f:\n",
    "    pickle.dump(keys_count, f)\n",
    "    \n",
    "with open('wiki-dump/key_to_index', 'wb') as f:\n",
    "    pickle.dump(key_to_index, f)\n",
    "\n",
    "# 114.47 sec, ngram=(1, 20) 1,000\n",
    "# 86.39 sec, ngram=(1, 16) 1,000\n",
    "# 47.87 sec, ngram=(1, 10) 1,000\n",
    "# 406.82 sec, ngram=(1, 16) 5,000\n",
    "# 843.35 sec, ngram=(1, 16) 10,000\n",
    "# 3443.72 sec, ngram=(1, 16) iterando de a 10,000\n",
    "# 4310.21 sec, ngram=(1, 21) iterando de a 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(keys_count[0, 52156])\n",
    "print(vectorizer.get_feature_names()[2])\n",
    "key_to_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ft[-2])\n",
    "print(vectorizer.vocabulary_['caught in the act'])\n",
    "print(keys_count[0, 1561442])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "Ranking method, Keyphraseness:\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyphraseness = np.zeros((1, len(vocabulary.keys())))\n",
    "\n",
    "for index, keyword in enumerate(vectorizer.get_feature_names()):\n",
    "    keyphraseness[0, index] = vocabulary[keyword] / keys_count[0, index]\n",
    "\n",
    "# Save data\n",
    "with open('wiki-dump/keyphraseness', 'wb') as f:\n",
    "    pickle.dump(keyphraseness, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "------------------------\n",
    "Evaluation of Keyphraseness:\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "    \n",
    "with open('wiki-dump/keyphraseness', 'rb') as f:\n",
    "    keyphraseness = pickle.load(f)\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/mini/enwiki-train', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "count = 1000\n",
    "articles = []\n",
    "for event, elem in context:\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = iterator.__next__().text\n",
    "    \n",
    "    if len(articles) < count:\n",
    "        # Extract data only over articles, exclude: talks, mediawiki, portal, user, etc\n",
    "        if namespace_id == ARTICLE_ID:\n",
    "            try:\n",
    "                iterator = elem.iterchildren(tag=REDIRECT_TAG)\n",
    "                redirect_child = iterator.__next__()\n",
    "\n",
    "            except StopIteration:\n",
    "                iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "                text = iterator.__next__().text.lower()\n",
    "                articles.append(text)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_pattern = '\\[\\[([^][]+)\\]\\]'\n",
    "extract_regex = re.compile(extract_pattern, re.IGNORECASE)\n",
    "ignore = ['Image:', 'File:', 'Category:', 'Q:', 'image:', 'file:', 'category:', 'q:']\n",
    "\n",
    "for article in articles:\n",
    "    # Gold keyphrases extracted for wikipedia articles\n",
    "    gold_keyphrases = set([key.split('|')[-1].lower() for key in extract_regex.findall(article)\n",
    "                           if not any(x in key for x in ignore) and len(key) > 0])\n",
    "    \n",
    "    # Predicted keyphrases\n",
    "    vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 16), \n",
    "                                 vocabulary=vocabulary.keys(), token_pattern='[\\S]+')\n",
    "    m = vectorizer.transform([article])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article = articles[0]\n",
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 16), vocabulary=vocabulary.keys())\n",
    "m = vectorizer.transform([article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows, cols = m.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zipped = list(zip(cols, keyphraseness[0, cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_6 = int(len(article.split(' ')) * .06)\n",
    "percent_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top = nlargest(percent_6, zipped, key=lambda t: t[1])\n",
    "# top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_pattern = '\\[\\[([^][]+)\\]\\]'\n",
    "extract_regex = re.compile(extract_pattern, re.IGNORECASE)\n",
    "ignore = ['Image:', 'File:', 'Category:', 'Q:', 'image:', 'file:', 'category:', 'q:']\n",
    "\n",
    "gold_keyphrases = set([key.split('|')[-1].lower() for key in extract_regex.findall(article)\n",
    "                       if not any(x in key for x in ignore) and len(key) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_names = vectorizer.get_feature_names()\n",
    "predicted_keyphrases = [features_names[i] for i, prob in top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ernest hemingway',\n",
       " 'gene kelly',\n",
       " 'leonard bernstein',\n",
       " 'new york',\n",
       " 'new york philharmonic',\n",
       " 'pablo picasso',\n",
       " 'piccolo',\n",
       " 'snare drum',\n",
       " 'soprano saxophone',\n",
       " 'triangle',\n",
       " 'university of michigan'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_keyphrases.intersection(set(predicted_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classical'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_names[140033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instrument',\n",
       " 'conjunction',\n",
       " 'she',\n",
       " 'play',\n",
       " 'poem',\n",
       " 'others',\n",
       " 'kennedy',\n",
       " 'leonard bernstein',\n",
       " 'ira gershwin',\n",
       " 'victor',\n",
       " 'soprano saxophone',\n",
       " 'loose',\n",
       " 'de',\n",
       " 'modern',\n",
       " 'four',\n",
       " 'angeles',\n",
       " 'he',\n",
       " 'lc',\n",
       " 'los angeles',\n",
       " 'calendar',\n",
       " 'tone poem',\n",
       " 'chaplin',\n",
       " 'gene kelly',\n",
       " 'horns',\n",
       " 'picasso',\n",
       " 'artist',\n",
       " 'new york philharmonic',\n",
       " 'radio',\n",
       " 'new york',\n",
       " 'first episode',\n",
       " 'rca',\n",
       " 'the united states',\n",
       " 'composer',\n",
       " 'all',\n",
       " 'war',\n",
       " 'net',\n",
       " 'walking',\n",
       " 'slow',\n",
       " 'pablo picasso',\n",
       " 'states',\n",
       " 'impressions',\n",
       " 'caron',\n",
       " 'place',\n",
       " 'document',\n",
       " 'to',\n",
       " 'web',\n",
       " 'working',\n",
       " 'master',\n",
       " 'compiled',\n",
       " 'ami',\n",
       " 'california',\n",
       " 'bowl',\n",
       " 'pound',\n",
       " 'snare drum',\n",
       " 'orchestra',\n",
       " 'ernest',\n",
       " 'hand',\n",
       " 'when',\n",
       " 'william',\n",
       " 'gershwin',\n",
       " 'back',\n",
       " 'the street',\n",
       " 'last',\n",
       " 'wood',\n",
       " 'thumb',\n",
       " 'multiple',\n",
       " 'complete',\n",
       " 'prior',\n",
       " '22',\n",
       " 'theories',\n",
       " 'fame',\n",
       " 'audience',\n",
       " 'portrait',\n",
       " 'premiere',\n",
       " '30',\n",
       " 'one',\n",
       " 'recorded',\n",
       " 'triangle',\n",
       " 'made',\n",
       " 'plus',\n",
       " 'http',\n",
       " 'episode',\n",
       " 'the sounds',\n",
       " 'style',\n",
       " 'memorial',\n",
       " 'c√©sar',\n",
       " 're',\n",
       " 'conductor',\n",
       " 'taylor',\n",
       " 'little',\n",
       " 'university of michigan',\n",
       " 'hosted',\n",
       " 'applause',\n",
       " '40',\n",
       " 'block',\n",
       " 'do',\n",
       " 'advice',\n",
       " 'piccolo',\n",
       " '29',\n",
       " 'year',\n",
       " 'we',\n",
       " 'this time',\n",
       " 'andante',\n",
       " 'low',\n",
       " 'process',\n",
       " 'accepted',\n",
       " 'work',\n",
       " 'cd',\n",
       " 'so',\n",
       " 'its',\n",
       " 'may',\n",
       " 'ernest hemingway']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
