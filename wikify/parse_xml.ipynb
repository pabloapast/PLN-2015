{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "from lxml import etree\n",
    "from sys import getsizeof\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "NAMESPACE = '{http://www.mediawiki.org/xml/export-0.10/}'\n",
    "PAGE_TAG = NAMESPACE + 'page'\n",
    "TITLE_TAG = NAMESPACE + 'title'\n",
    "REDIRECT_TAG = NAMESPACE + 'redirect'\n",
    "TEXT_TAG = NAMESPACE + 'text'\n",
    "SOURCE = './wiki-dump/enwiki-1gb'\n",
    "DEST = './wiki-dump/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Utils:\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iteration over xml with low ram consumption\n",
    "def fast_iter(context, func, dest):\n",
    "    for event, elem in context:\n",
    "        # Execute operations over xml node\n",
    "        stop = func(elem, dest)\n",
    "        \n",
    "        # Clear data read\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "        \n",
    "        if stop:\n",
    "            break\n",
    "    \n",
    "    del context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------------\n",
    "Split wikipedia dump xml:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "xml_header = '<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\\n'\n",
    "xml_tail = '</mediawiki>\\n'\n",
    "\n",
    "n = 2\n",
    "name = './wiki-dump/enwiki-part'\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_header)\n",
    "\n",
    "# Splits wikipedia dump into n parts choosing randomly the articles of each part\n",
    "for event, elem in context:\n",
    "    # Execute operations over xml node\n",
    "    temp_prob = 0\n",
    "    rand = random()\n",
    "    for i in range(1, n + 1):\n",
    "        if temp_prob <= rand < i / n:\n",
    "            filename = name + str(i)\n",
    "            with open(filename, 'ab') as f:\n",
    "                wiki_page = etree.tostring(elem)\n",
    "                f.write(wiki_page)\n",
    "            break\n",
    "        else:\n",
    "            temp_prob = i / n\n",
    "    \n",
    "    # Clear data read\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_tail)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "Extract keyphrases:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean_pattern = '\\[\\[(image:|file:|category:)(.+)\\]\\]'\n",
    "# clean_pattern2 = '\\[\\[(image:|file:|category:)(.*(<br>\\n)*)*]]'\n",
    "extract_pattern = '\\[\\[([^][]+)\\]\\]'\n",
    "\n",
    "ignore = ['Image:', 'File:', 'Category:', 'Q:', 'image:', 'file:', 'category:', 'q:']\n",
    "\n",
    "# clean_regex = re.compile(clean_pattern, re.IGNORECASE)\n",
    "extract_regex = re.compile(extract_pattern, re.IGNORECASE)\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "vocabulary = Counter()\n",
    "for event, elem in context:\n",
    "    new_keys = []\n",
    "    # Extract titles of pages\n",
    "    try:\n",
    "        iterator = elem.iterchildren(tag=REDIRECT_TAG)\n",
    "        redirect_child = iterator.__next__()\n",
    "        page_title = redirect_child.attrib['title'].lower()\n",
    "\n",
    "    except StopIteration:\n",
    "        iterator = elem.iterchildren(tag=TITLE_TAG)\n",
    "        title_child = iterator.__next__()\n",
    "        page_title = title_child.text.lower()\n",
    "        \n",
    "    new_keys.append(page_title)\n",
    "\n",
    "    # Extract keyphrases in other morphological form\n",
    "    iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "    text_child = iterator.__next__()\n",
    "    if text_child.text is not None:\n",
    "#         keyphrases = clean_regex.sub('', text_child.text)\n",
    "        keyphrases = extract_regex.findall(text_child.text)\n",
    "        for key in keyphrases:\n",
    "            if not any(x in key for x in ignore) and len(key) > 0:\n",
    "                new_keys.append(key.split('|')[-1].lower())\n",
    "    \n",
    "    vocabulary.update(new_keys)\n",
    "\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "\n",
    "print('Vocabulary: {} ngrams --- {} MB'.format(len(vocabulary), getsizeof(vocabulary) / 1024**2))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "# Save vocabulary\n",
    "with open('./wiki-dump/vocabulary_dict', 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('./wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "#     vocabulary = pickle.load(f)\n",
    "\n",
    "print(len(vocabulary.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_500 = nlargest(500, vocabulary.keys(), key=lambda x: len(x.split()))\n",
    "print([len(i.split()) for i in top_500])\n",
    "# sum([len(i.split()) for i in vocabulary.keys()]) / len(vocabulary)  # ==> 2.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------------------\n",
    "Count N-grams in articles\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3443.727946996689 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with open('./wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "vec = CountVectorizer(lowercase=True, ngram_range=(1, 16), vocabulary=vocabulary.keys(), token_pattern='[\\S]+')\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=TEXT_TAG , events=('end',))\n",
    "articles = []\n",
    "keys_count = np.zeros((1, len(vocabulary.keys())))\n",
    "\n",
    "for i, (event, elem) in enumerate(context):\n",
    "    if i % 100 == 0:\n",
    "        m = vec.transform(articles)\n",
    "        m = m.sum(axis=0)\n",
    "        keys_count = np.add(keys_count, m)\n",
    "        articles = []\n",
    "    elif i == 501:\n",
    "        break\n",
    "    else:\n",
    "        articles.append(elem.text)\n",
    "\n",
    "    # Clear data read\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "for index, keyword in enumerate(vec.get_feature_names()):\n",
    "    keys_count[0, index] += vocabulary[keyword]\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "with open('wiki-dump/keys_count', 'wb') as f:\n",
    "    pickle.dump(keys_count, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caught in the act\n",
      "1561442\n",
      "18.0\n"
     ]
    }
   ],
   "source": [
    "print(ft[-2])\n",
    "print(vec.vocabulary_['caught in the act'])\n",
    "print(keys_count[0, 1561442])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyphraseness = np.zeros((1, len(vocabulary.keys())))\n",
    "\n",
    "for index, keyword in enumerate(vec.get_feature_names()):\n",
    "    keyphraseness[0, index] = vocabulary[keyword] / keys_count[0, index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.88888889,  1.        , ...,  1.        ,\n",
       "         0.16666667,  1.        ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphraseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text(elem, dest):\n",
    "    stop = False\n",
    "    dest.append(elem.text)\n",
    "    \n",
    "    if len(dest) >= 10000:\n",
    "        stop = True\n",
    "        \n",
    "    return stop\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=TEXT_TAG , events=('end',))\n",
    "articles = []\n",
    "\n",
    "fast_iter(context, get_text, articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "vec = CountVectorizer(lowercase=True, ngram_range=(1, 16), vocabulary=vocabulary.keys(), token_pattern='[\\S]+')\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "m = vec.transform(articles)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# 114.47 sec, ngram=(1, 20) 1,000\n",
    "# 86.39 sec, ngram=(1, 16) 1,000\n",
    "# 47.87 sec, ngram=(1, 10) 1,000\n",
    "# 406.82 sec, ngram=(1, 16) 5,000\n",
    "# 843.35 sec, ngram=(1, 16) 10,000\n",
    "# 3443.72 sec, ngram=(1, 16) iterando de a 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
