{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "from lxml import etree\n",
    "from sys import getsizeof\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NAMESPACE = '{*}'\n",
    "ARTICLE_ID = '0'\n",
    "NAMESPACE_TAG = NAMESPACE + 'ns'\n",
    "PAGE_TAG = NAMESPACE + 'page'\n",
    "TITLE_TAG = NAMESPACE + 'title'\n",
    "REDIRECT_TAG = NAMESPACE + 'redirect'\n",
    "TEXT_TAG = NAMESPACE + 'text'\n",
    "SOURCE = './wiki-dump/enwiki-1gb'\n",
    "DEST = './wiki-dump/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Utils:\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iteration over xml with low ram consumption\n",
    "def fast_iter(context, func, dest):\n",
    "    for event, elem in context:\n",
    "        # Execute operations over xml node\n",
    "        stop = func(elem, dest)\n",
    "        \n",
    "        # Clear data read\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "        \n",
    "        if stop:\n",
    "            break\n",
    "    \n",
    "    del context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------------\n",
    "Split wikipedia dump xml:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "xml_header = '<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\\n'\n",
    "xml_tail = '</mediawiki>\\n'\n",
    "\n",
    "n = 2\n",
    "name = './wiki-dump/enwiki-part'\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_header)\n",
    "\n",
    "# Splits wikipedia dump into n parts choosing randomly the articles of each part\n",
    "for event, elem in context:\n",
    "    # Execute operations over xml node\n",
    "    temp_prob = 0\n",
    "    rand = random()\n",
    "    for i in range(1, n + 1):\n",
    "        if temp_prob <= rand < i / n:\n",
    "            filename = name + str(i)\n",
    "            with open(filename, 'ab') as f:\n",
    "                wiki_page = etree.tostring(elem)\n",
    "                f.write(wiki_page)\n",
    "            break\n",
    "        else:\n",
    "            temp_prob = i / n\n",
    "    \n",
    "    # Clear data read\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_tail)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "Extract keywords:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean_pattern = '\\[\\[(image:|file:|category:)(.+)\\]\\]'\n",
    "# clean_pattern2 = '\\[\\[(image:|file:|category:)(.*(<br>\\n)*)*]]'\n",
    "extract_pattern = '\\[\\[([^][]+)\\]\\]'\n",
    "\n",
    "ignore = ['Image:', 'File:', 'Category:', 'Q:', 'Wikipedia:', 'image:', 'file:', 'category:', 'q:', 'wikipedia:']\n",
    "          #, '<span style=', 'talk:', 'Talk:', '<font style=']\n",
    "\n",
    "# clean_regex = re.compile(clean_pattern, re.IGNORECASE)\n",
    "extract_regex = re.compile(extract_pattern, re.IGNORECASE)\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/mini/enwiki-test1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "vocabulary = Counter()\n",
    "for event, elem in context:\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = iterator.__next__().text\n",
    "    \n",
    "    # Extract data only over articles, exclude: talks, mediawiki, portal, user, etc\n",
    "    if namespace_id == ARTICLE_ID:\n",
    "        new_keys = []\n",
    "        # Extract titles of pages\n",
    "        try:\n",
    "            iterator = elem.iterchildren(tag=REDIRECT_TAG)\n",
    "            redirect_child = iterator.__next__()\n",
    "            page_title = redirect_child.attrib['title'].lower()\n",
    "\n",
    "        except StopIteration:\n",
    "            iterator = elem.iterchildren(tag=TITLE_TAG)\n",
    "            title_child = iterator.__next__()\n",
    "            page_title = title_child.text.lower()\n",
    "\n",
    "        new_keys.append(page_title)\n",
    "\n",
    "        # Extract keyphrases in other morphological form\n",
    "        iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "        text = iterator.__next__().text\n",
    "        keyphrases = extract_regex.findall(text)\n",
    "        for key in keyphrases:\n",
    "            if not any(x in key for x in ignore) and len(key) > 0:\n",
    "                new_keys.append(key.split('|')[-1].lower())\n",
    "\n",
    "        vocabulary.update(new_keys)\n",
    "\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "# Delete keyphrases that occurs less than 5 times\n",
    "for key, value in list(vocabulary.items()):\n",
    "    if value <= 5:\n",
    "        del vocabulary[key]\n",
    "\n",
    "\n",
    "print('Vocabulary: {} ngrams --- {} MB'.format(len(vocabulary), getsizeof(vocabulary) / 1024**2))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "# Save vocabulary\n",
    "with open('./wiki-dump/vocabulary_dict', 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)\n",
    "\n",
    "# Vocabulary: 204163 ngrams - 97.37 sec - enwiki-test1gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_10 = nlargest(10, vocabulary.keys(), key=lambda x: len(x.split()))\n",
    "# print(top_100)\n",
    "# print([len(i.split()) for i in top_10])\n",
    "# print(sum([len(i.split()) for i in vocabulary.keys()]) / len(vocabulary))  # ==> 2.09\n",
    "top_10[-5:]\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------------------\n",
    "Count N-grams in articles:\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with open('./wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 16), vocabulary=vocabulary.keys(), token_pattern='[\\S]+')\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/mini/enwiki-test1gb', tag=PAGE_TAG , events=('end',))\n",
    "articles = []\n",
    "keys_count = np.zeros((1, len(vocabulary.keys())))\n",
    "\n",
    "for i, (event, elem) in enumerate(context):\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = iterator.__next__().text\n",
    "\n",
    "    if namespace_id == ARTICLE_ID:\n",
    "        if i % 10000 == 0:\n",
    "            m = vectorizer.transform(articles)\n",
    "            m = m.sum(axis=0)\n",
    "            keys_count = np.add(keys_count, m)\n",
    "            articles = []\n",
    "        else:\n",
    "            iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "            text = iterator.__next__().text\n",
    "            articles.append(text)\n",
    "\n",
    "    # Clear data read\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "\n",
    "# Leave this or not?\n",
    "# I take advantage of this loop to build a dict from vocabulary keys to their index in the array\n",
    "key_to_index = dict()\n",
    "for index, keyword in enumerate(vectorizer.get_feature_names()):\n",
    "    keys_count[0, index] += vocabulary[keyword]\n",
    "    key_to_index[keyword] = index\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Save data\n",
    "# with open('wiki-dump/vectorizer', 'wb') as f:\n",
    "#     pickle.dump(vectorizer, f)\n",
    "\n",
    "with open('wiki-dump/keys_count', 'wb') as f:\n",
    "    pickle.dump(keys_count, f)\n",
    "    \n",
    "with open('wiki-dump/key_to_index', 'wb') as f:\n",
    "    pickle.dump(key_to_index, f)\n",
    "\n",
    "# 114.47 sec, ngram=(1, 20) 1,000\n",
    "# 86.39 sec, ngram=(1, 16) 1,000\n",
    "# 47.87 sec, ngram=(1, 10) 1,000\n",
    "# 406.82 sec, ngram=(1, 16) 5,000\n",
    "# 843.35 sec, ngram=(1, 16) 10,000\n",
    "# 3443.72 sec, ngram=(1, 16) iterando de a 10,000\n",
    "# 4310.21 sec, ngram=(1, 21) iterando de a 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(keys_count[0, 52156])\n",
    "print(vectorizer.get_feature_names()[2])\n",
    "key_to_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ft[-2])\n",
    "print(vectorizer.vocabulary_['caught in the act'])\n",
    "print(keys_count[0, 1561442])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "Ranking method, Keyphraseness:\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyphraseness = np.zeros((1, len(vocabulary.keys())))\n",
    "\n",
    "for index, keyword in enumerate(vectorizer.get_feature_names()):\n",
    "    keyphraseness[0, index] = vocabulary[keyword] / keys_count[0, index]\n",
    "\n",
    "# Save data\n",
    "with open('wiki-dump/keyphraseness', 'wb') as f:\n",
    "    pickle.dump(keyphraseness, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "------------------------\n",
    "Evaluation of Keyphraseness:\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "    \n",
    "with open('wiki-dump/keyphraseness', 'rb') as f:\n",
    "    keyphraseness = pickle.load(f)\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/mini/enwiki-train', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "count = 1000\n",
    "articles = []\n",
    "for event, elem in context:\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = iterator.__next__().text\n",
    "    \n",
    "    if len(articles) < count:\n",
    "        # Extract data only over articles, exclude: talks, mediawiki, portal, user, etc\n",
    "        if namespace_id == ARTICLE_ID:\n",
    "            try:\n",
    "                iterator = elem.iterchildren(tag=REDIRECT_TAG)\n",
    "                redirect_child = iterator.__next__()\n",
    "\n",
    "            except StopIteration:\n",
    "                iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "                text = iterator.__next__().text.lower()\n",
    "                articles.append(text)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_pattern = '\\[\\[([^][]+)\\]\\]'\n",
    "extract_regex = re.compile(extract_pattern, re.IGNORECASE)\n",
    "ignore = ['Image:', 'File:', 'Category:', 'Q:', 'image:', 'file:', 'category:', 'q:']\n",
    "\n",
    "for article in articles:\n",
    "    # Gold keyphrases extracted for wikipedia articles\n",
    "    gold_keyphrases = set([key.split('|')[-1].lower() for key in extract_regex.findall(article)\n",
    "                           if not any(x in key for x in ignore) and len(key) > 0])\n",
    "    \n",
    "    # Predicted keyphrases\n",
    "    vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 16), \n",
    "                                 vocabulary=vocabulary.keys(), token_pattern='[\\S]+')\n",
    "    m = vectorizer.transform([article])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article = articles[0]\n",
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 16), vocabulary=vocabulary.keys())\n",
    "m = vectorizer.transform([article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows, cols = m.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zipped = list(zip(cols, keyphraseness[0, cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_6 = int(len(article.split(' ')) * .06)\n",
    "percent_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top = nlargest(percent_6, zipped, key=lambda t: t[1])\n",
    "# top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_pattern = '\\[\\[([^][]+)\\]\\]'\n",
    "extract_regex = re.compile(extract_pattern, re.IGNORECASE)\n",
    "ignore = ['Image:', 'File:', 'Category:', 'Q:', 'image:', 'file:', 'category:', 'q:']\n",
    "\n",
    "gold_keyphrases = set([key.split('|')[-1].lower() for key in extract_regex.findall(article)\n",
    "                       if not any(x in key for x in ignore) and len(key) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_names = vectorizer.get_feature_names()\n",
    "predicted_keyphrases = [features_names[i] for i, prob in top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gold_keyphrases.intersection(set(predicted_keyphrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_names[140033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary['22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from nltk import regexp_tokenize\n",
    "\n",
    "# pattern = r'''(?ix)    # set flag to allow verbose regexps\n",
    "#      ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "#    | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "#    | \\w+(-\\w+)*        # words with optional internal hyphens\n",
    "#    | \\.\\.\\.            # ellipsis\n",
    "#    | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\n",
    "# '''\n",
    "pattern = r''' (?x)         # set flag to allow verbose regexps \n",
    "    ([A-Z]\\.)+          # abbreviations (e.g. U.S.A.)\n",
    "    | \\w+(-\\w+)*        # words with optional internal hyphens\n",
    "    | \\$?\\d+(\\.\\d+)?%?  # currency & percentages\n",
    "    | \\.\\.\\.            # ellipses '''\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=partial(regexp_tokenize, pattern=pattern))\n",
    "# m = vectorizer.fit_transform('I love N.Y.C. 100% even with all of its traffic-ridden streets...')\n",
    "# m = vectorizer.transform(['sale la flÃ²r. Y [[el-gato]]', 'estaba el-gato'])\n",
    "\n",
    "# print(vectorizer.get_feature_names())\n",
    "# print(m.toarray())\n",
    "vectorizer.build_analyzer()('I love N.Y.C. 100% even with all of its traffic-ridden streets...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "voc = ['el gato', \"blink 's\"]\n",
    "vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 6), vocabulary=voc, tokenizer=word_tokenize)\n",
    "text = [\n",
    "    'estaba [[el gato]].',\n",
    "    \"toco '''blink's'''\",\n",
    "]\n",
    "\n",
    "m = vectorizer.transform(text)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(m.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_tokenize(\"El GATO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 79.41121578216553 sec: extract_keywords ---\n"
     ]
    }
   ],
   "source": [
    "# Ignore keywords starting with this names\n",
    "_ignored_keywords = ['image:', 'file:', 'category:', 'wikipedia:']\n",
    "# Regular expression used to extract keywords inside '[[ ]]'\n",
    "_extract_regex = re.compile('\\[\\[([^][]+)\\]\\]', re.IGNORECASE)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = [token for token in word_tokenize(text)\n",
    "              if not token in punctuation]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def fast_xml_iter(context, func, dest):\n",
    "    for event, elem in context:\n",
    "        # Execute operations over xml node\n",
    "        func(elem, dest)\n",
    "#         break\n",
    "        # Clear data read\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "            \n",
    "\n",
    "def extract_keywords(elem, dest):\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = next(iterator).text\n",
    "\n",
    "    if namespace_id == ARTICLE_ID:\n",
    "        keywords = []\n",
    "        # Text in the article\n",
    "        iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "        text = next(iterator).text\n",
    "        # Find words inside '[[ ]]'\n",
    "        words = _extract_regex.findall(text)\n",
    "\n",
    "        for word in words:\n",
    "            word = clean_text(word.split('|')[-1])\n",
    "            if not any(x in word for x in _ignored_keywords) and len(word) > 0:\n",
    "                keywords.append(word)\n",
    "\n",
    "        dest.update(keywords)            \n",
    "\n",
    "\n",
    "# Iterates over xml and extract keywords\n",
    "start_time = time.time()\n",
    "\n",
    "xml_iterator = etree.iterparse('./wiki-dump/mini/enwiki-test1gb', tag=PAGE_TAG)\n",
    "# Vocabulary of keywords with their occurrence count\n",
    "_vocabulary = Counter()\n",
    "# fast_xml_iter(xml_iterator, extract_keywords, _vocabulary)\n",
    "for event, elem in xml_iterator:\n",
    "    iterator = elem.iterchildren(tag=NAMESPACE_TAG)\n",
    "    namespace_id = next(iterator).text\n",
    "\n",
    "    if namespace_id == ARTICLE_ID:\n",
    "        keywords = []\n",
    "        # Text in the article\n",
    "        iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "        text = next(iterator).text\n",
    "        # Find words inside '[[ ]]'\n",
    "        words = _extract_regex.findall(text)\n",
    "\n",
    "        for word in words:\n",
    "#             word = clean_text(word.split('|')[-1])\n",
    "            word = word.split('|')[-1]\n",
    "            if not any(x in word for x in _ignored_keywords) and len(word) > 0:\n",
    "                keywords.append(word)\n",
    "\n",
    "        _vocabulary.update(keywords)\n",
    "        \n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "del xml_iterator\n",
    "print(\"--- %s sec: extract_keywords ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
