{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from collections import Counter\n",
    "from io import StringIO, BytesIO\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "from sys import getsizeof\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "NAMESPACE = '{http://www.mediawiki.org/xml/export-0.10/}'\n",
    "PAGE_TAG = NAMESPACE + 'page'\n",
    "TITLE_TAG = NAMESPACE + 'title'\n",
    "REDIRECT_TAG = NAMESPACE + 'redirect'\n",
    "TEXT_TAG = NAMESPACE + 'text'\n",
    "SOURCE = './wiki-dump/enwiki-1gb'\n",
    "DEST = './wiki-dump/'\n",
    "\n",
    "\n",
    "# Fast iteration over xml with low ram consumption\n",
    "def fast_iter(context, func, dest):\n",
    "    for event, elem in context:\n",
    "        # Execute operations over xml node\n",
    "        func(elem, dest)\n",
    "        # Clear data read\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "    del context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXTRAER TITULOS NAIVE\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', events=('end',))\n",
    "\n",
    "titles = []\n",
    "\n",
    "for event, elem in context:\n",
    "    if 'title' in elem.tag:\n",
    "        if elem.text is not None:\n",
    "            titles.append(elem.text.lower())\n",
    "#             titles.append({'title': elem.text.lower(), 'redirect': ''})\n",
    "#             titles.append((elem.text,))\n",
    "    elif 'redirect' in elem.tag:\n",
    "        titles[-1] = elem.attrib['title'].lower()\n",
    "#         titles[-1]['redirect'] = elem.attrib['title'].lower()\n",
    "#         titles[-1] = titles[-1] + (elem.attrib['title'],)\n",
    "    \n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "print('Titles: {} --- {} MB'.format(len(titles), getsizeof(titles) / 1024**2))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with open('./wiki-dump/titles', 'w') as outfile:\n",
    "    json.dump(titles, outfile, indent=2)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EXTRAER TITULOS: ALGORITMO ESTRELLA :D\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "titles = set()\n",
    "for event, elem in context:\n",
    "    try:\n",
    "        iter_child = elem.iterchildren(tag=REDIRECT_TAG)\n",
    "        redirect_child = iter_child.__next__()\n",
    "#         titles.append(redirect_child.attrib['title'].lower())\n",
    "        titles.add(redirect_child.attrib['title'].lower())\n",
    "\n",
    "    except StopIteration:\n",
    "        iter_child = elem.iterchildren(tag=TITLE_TAG)\n",
    "        title_child = iter_child.__next__()\n",
    "#         titles.append(title_child.text.lower())\n",
    "        titles.add(title_child.text.lower())\n",
    "        \n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "\n",
    "print('Titles: {} --- {} MB'.format(len(titles), getsizeof(titles) / 1024**2))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repeated = titles\n",
    "from collections import Counter\n",
    "a = dict(Counter(repeated))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 1000\n",
    "for event, elem in etree.iterparse('./wiki-dump/enwiki-1gb', events=('start', 'end')):\n",
    "    if event == 'start' and 'page' in elem.tag:\n",
    "        f = elem.find('{http://www.mediawiki.org/xml/export-0.10/}redirect')\n",
    "        if f is not None:\n",
    "            print(f.attrib['title'])\n",
    "        else:\n",
    "            f = elem.find('{http://www.mediawiki.org/xml/export-0.10/}title')\n",
    "            print(f.text)\n",
    "    if count <= 0:\n",
    "        break\n",
    "    count -= 1\n",
    "    elem.clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 170\n",
    "text = ''\n",
    "for event, elem in etree.iterparse('./wiki-dump/enwiki-1gb', events=('start', 'end')):\n",
    "    if event == 'start' and 'text' in elem.tag:\n",
    "        count -= 1\n",
    "        if count == 0:\n",
    "            text = elem.text\n",
    "            break\n",
    "\n",
    "# m = re.findall('\\[\\[(.*?)]]', text)\n",
    "m = re.findall('\\[\\[([^][]+)]]', text)\n",
    "print(len(m))\n",
    "print(m)\n",
    "\n",
    "for i, s in enumerate(m):\n",
    "    m[i] = s.split('|')[-1]\n",
    "\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context1 = etree.iterparse('./wiki-dump/enwiki-1gb', tag=PAGE_TAG , events=('end',))\n",
    "context2 = etree.iterparse('./wiki-dump/enwiki-1gb', tag=TEXT_TAG , events=('end',))\n",
    "\n",
    "count1 = 0\n",
    "for event, elem in context1:\n",
    "    count1 += 1\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "    \n",
    "count2 = 0\n",
    "for event, elem in context2:\n",
    "    count2 += 1\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context1  \n",
    "del context2  \n",
    "    \n",
    "print(count1, count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "alternative_keyphrases = set()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# with open('./wiki-dump/titles', 'wa') as f:\n",
    "\n",
    "# Extract keyphrases from articles\n",
    "for event, elem in etree.iterparse('./wiki-dump/enwiki-20160113-pages-articles.xml', events=('start', 'end')):\n",
    "    title = ''\n",
    "    # Extract titles of pages\n",
    "    if event == 'start' and 'title' in elem.tag:\n",
    "        if elem.text is not None:\n",
    "#                 vocabulary.append(elem.text.lower())\n",
    "#                 title = elem.text.lower()\n",
    "            pass\n",
    "\n",
    "    elif event == 'start' and 'redirect' in elem.tag:\n",
    "#             vocabulary[-1] = elem.attrib['title'].lower()\n",
    "#             title = elem.attrib['title'].lower()\n",
    "        pass\n",
    "\n",
    "    # Extract keyphrases in other morphological form\n",
    "    elif event == 'start' and 'text' in elem.tag:\n",
    "        text = elem.text\n",
    "        if text is not None:\n",
    "#                 keyphrases = re.findall('\\[\\[([^][]+)]]', text)\n",
    "#                 for key in keyphrases:\n",
    "#                     alternative_keyphrases.add(key.split('|')[-1].lower())\n",
    "            pass\n",
    "\n",
    "    elem.clear()\n",
    "        \n",
    "    \n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "vocabulary = alternative_keyphrases.union(set(vocabulary))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "print('Vocabulary: ', len(vocabulary))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------------\n",
    "Split wiki dump xml:\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "xml_header = '<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.10/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\">\\n'\n",
    "xml_tail = '</mediawiki>\\n'\n",
    "\n",
    "n = 2\n",
    "name = './wiki-dump/enwiki-part'\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_header)\n",
    "\n",
    "# Splits wikipedia dump into n parts choosing randomly the articles of each part\n",
    "for event, elem in context:\n",
    "    # Execute operations over xml node\n",
    "    temp_prob = 0\n",
    "    rand = random()\n",
    "    for i in range(1, n + 1):\n",
    "        if temp_prob <= rand < i / n:\n",
    "            filename = name + str(i)\n",
    "            with open(filename, 'ab') as f:\n",
    "                wiki_page = etree.tostring(elem)\n",
    "                f.write(wiki_page)\n",
    "            break\n",
    "        else:\n",
    "            temp_prob = i / n\n",
    "    \n",
    "    # Clear data read\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "for i in range(1, n + 1):\n",
    "    filename = name + str(i)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(xml_tail)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "Extract keyphrases:\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean_pattern = '\\[\\[(image:|file:|category:)(.+)\\]\\]'\n",
    "# clean_pattern2 = '\\[\\[(image:|file:|category:)(.*(<br>\\n)*)*]]'\n",
    "extract_pattern = '\\[\\[([^][]+)\\]\\]'\n",
    "\n",
    "ignore = ['Image:', 'File:', 'Category:', 'Q:', 'image:', 'file:', 'category:', 'q:']\n",
    "\n",
    "# clean_regex = re.compile(clean_pattern, re.IGNORECASE)\n",
    "extract_regex = re.compile(extract_pattern, re.IGNORECASE)\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb',\n",
    "                          tag=PAGE_TAG , events=('end',))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "vocabulary = Counter()\n",
    "# double_key = set()\n",
    "for event, elem in context:\n",
    "    new_keys = []\n",
    "    # Extract titles of pages\n",
    "    try:\n",
    "        iterator = elem.iterchildren(tag=REDIRECT_TAG)\n",
    "        redirect_child = iterator.__next__()\n",
    "        new_keys.append(redirect_child.attrib['title'].lower())\n",
    "\n",
    "    except StopIteration:\n",
    "        iterator = elem.iterchildren(tag=TITLE_TAG)\n",
    "        title_child = iterator.__next__()\n",
    "        new_keys.append(title_child.text.lower())\n",
    "\n",
    "    # Extract keyphrases in other morphological form\n",
    "    iterator = elem.iterdescendants(tag=TEXT_TAG)\n",
    "    text_child = iterator.__next__()\n",
    "    if text_child.text is not None:\n",
    "#         keyphrases = clean_regex.sub('', text_child.text)\n",
    "        keyphrases = extract_regex.findall(text_child.text)\n",
    "        for key in keyphrases:\n",
    "            if not any(x in key for x in ignore):\n",
    "                new_keys.append(key.split('|')[-1].lower())\n",
    "#                 if len(key.split('|')) > 1:\n",
    "#                     double_key.add(key.lower())\n",
    "    \n",
    "    vocabulary.update(new_keys)\n",
    "\n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "\n",
    "del context\n",
    "\n",
    "\n",
    "print('Vocabulary: {} ngrams --- {} MB'.format(len(vocabulary), getsizeof(vocabulary) / 1024**2))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "# Save vocabulary\n",
    "with open('./wiki-dump/vocabulary_dict', 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('./wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "#     voc = pickle.load(f)\n",
    "\n",
    "print(len(voc.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "top_20 = nlargest(20, vocabulary.keys(), key=len)\n",
    "print([len(i.split()) for i in top_20])\n",
    "# top_20[50:]\n",
    "# len(\"when the pawn hits the conflicts he thinks like a king what he knows throws the blows when he goes to the fight and he'll win the whole thing 'fore he enters the ring there's no body to batter when your mind is your might so when you go solo, you hold your own hand and remember that depth is the greatest of heights and if you know where you stand, then you know where to land and if you fall it won't matter, cuz you'll know that you're right}}\".split())\n",
    "# sum([len(i.split()) for i in vocabulary.keys()]) / len(vocabulary) = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------------------\n",
    "Count N-grams in articles\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from nltk.util import LazyMap\n",
    "\n",
    "# with open('./wiki-dump/vocabulary', 'rb') as f:\n",
    "#     voc = pickle.dump(f)\n",
    "\n",
    "# minivoc = ['titulo ,e', 'otro mas-va', 'simple', '3']\n",
    "\n",
    "# text = ['simple es el titulo ,e simple 2 y otro mas va',\n",
    "#         'segunda prueba',\n",
    "#         'otro mas-va',\n",
    "#         '2 SIMPLE 3',\n",
    "#        ]\n",
    "minivoc = ['màchear', 'no, anda']\n",
    "text = ['pará @ [[màchear]] no màchear', \n",
    "        'màchear màchear no, anda']\n",
    "\n",
    "# context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=TEXT_TAG , events=('end',))\n",
    "vec = CountVectorizer(lowercase=True, ngram_range=(1, 10), vocabulary=minivoc, token_pattern='[\\S]+')\n",
    "vec2 = CountVectorizer(lowercase=True, ngram_range=(1, 10), vocabulary=minivoc, token_pattern='\\[\\[([^][]+)\\]\\]')\n",
    "# vec.fit_transform([])\n",
    "\n",
    "m = vec.transform(text).toarray()\n",
    "print(vec.get_feature_names())\n",
    "print(m)\n",
    "\n",
    "m2 = vec2.transform(text).toarray()\n",
    "print(vec2.get_feature_names())\n",
    "print(m2)\n",
    "\n",
    "re.findall('\\[\\[([^][]+)\\]\\]', 'viendo si [[funca|es]] esto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text(elem):\n",
    "    text = elem.text\n",
    "    \n",
    "    elem.clear()\n",
    "    while elem.getprevious() is not None:\n",
    "        del elem.getparent()[0]\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "context = etree.iterparse('./wiki-dump/enwiki-1gb', tag=TEXT_TAG , events=('end',))\n",
    "\n",
    "articles = []\n",
    "count = 1000\n",
    "for event, elem in context:\n",
    "    articles.append(get_text(elem))\n",
    "    \n",
    "    count -= 1\n",
    "    if count <= 0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./wiki-dump/vocabulary_dict', 'rb') as f:\n",
    "    voc = pickle.load(f)\n",
    "\n",
    "vec = CountVectorizer(lowercase=True, ngram_range=(1, 20), vocabulary=set(voc.keys()), token_pattern='[\\S]+')\n",
    "\n",
    "# text = [get_text(elem) for event, elem in context]\n",
    "m = vec.transform(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'avenin',\n",
       " 'glorious twelfth',\n",
       " 'niigon',\n",
       " 'herman keiser',\n",
       " 'huangshan pine',\n",
       " 'if i ruled the world',\n",
       " 'generic security services application program interface',\n",
       " 'hollands kroon',\n",
       " \"upheld mega bloks' rights to sell their product\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(voc.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# s = m.sum(axis=0)\n",
    "ft_names = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ft_names[956])\n",
    "voc[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585 956\n",
      "35 972\n",
      "1 1000\n",
      "2 1012\n",
      "29 1020\n",
      "2 1026\n",
      "4 1027\n",
      "1 1044\n",
      "9 1082\n",
      "1 1109\n",
      "1 1125\n",
      "2 1158\n",
      "1 1184\n",
      "1 1193\n",
      "1 1207\n",
      "1 1232\n",
      "2 1371\n",
      "2 1493\n",
      "1 1556\n",
      "1 1570\n",
      "5 1604\n",
      "3 1631\n",
      "1 1670\n",
      "4 1762\n",
      "1 1836\n",
      "3 1893\n",
      "1 1917\n",
      "1 2000\n",
      "2 2060\n",
      "2 2096\n",
      "3 2156\n",
      "1 2170\n",
      "1 2196\n",
      "1 2259\n",
      "1 2410\n",
      "1 2510\n",
      "1 2511\n",
      "1 2546\n",
      "1 2670\n",
      "1 2798\n",
      "2 2809\n",
      "1 2835\n",
      "2 2874\n",
      "1 3075\n",
      "1 3207\n",
      "2 3307\n",
      "1 3314\n",
      "1 3338\n",
      "1 3731\n",
      "1 3732\n",
      "1 3738\n",
      "1 3838\n",
      "1 3849\n",
      "2 3909\n",
      "1 4008\n",
      "3 4011\n",
      "471 4024\n",
      "15 4026\n",
      "1 4031\n",
      "4 4032\n",
      "3 4035\n",
      "5 4144\n",
      "19 4145\n",
      "1 4157\n",
      "19 4164\n",
      "1 4168\n",
      "1 4170\n",
      "23 4190\n",
      "1924 4192\n",
      "169 4211\n",
      "4 4247\n",
      "9 4260\n",
      "2 4271\n",
      "2 4289\n",
      "12 4353\n",
      "7 4369\n",
      "2 4428\n",
      "2 4622\n",
      "6 4644\n",
      "1 4679\n",
      "1 4680\n",
      "1 4739\n",
      "8 4785\n",
      "1 4868\n",
      "1 4899\n",
      "6 4917\n",
      "7 4943\n",
      "1 5104\n",
      "1 5142\n",
      "2 5173\n",
      "2 5214\n",
      "6 5220\n",
      "1 5226\n",
      "2 5287\n",
      "1 5318\n",
      "20 5345\n",
      "1 5368\n",
      "2 5380\n",
      "4 5399\n",
      "2 5403\n",
      "1 5405\n",
      "2 5414\n",
      "4 5441\n",
      "1 5548\n",
      "13 5585\n",
      "1 5626\n",
      "36 5639\n",
      "1 5781\n",
      "20 5884\n",
      "2 5886\n",
      "4 5887\n",
      "5 5894\n",
      "1 6007\n",
      "1 6118\n",
      "27 6181\n",
      "1 6185\n",
      "4 6229\n",
      "1 6339\n",
      "1 6367\n",
      "1 6389\n",
      "2 6470\n",
      "3 6648\n",
      "1 6732\n",
      "3 6745\n",
      "5 6772\n",
      "1 6773\n",
      "2 6810\n",
      "4 6902\n",
      "1 6903\n",
      "1 6949\n",
      "1 6966\n",
      "9 7047\n",
      "1 7062\n",
      "1 7066\n",
      "2 7101\n",
      "13 7143\n",
      "1 7162\n",
      "1 7187\n",
      "1 7255\n",
      "1 7410\n",
      "1 7441\n",
      "2 7475\n",
      "5 7482\n",
      "1 7486\n",
      "2 7497\n",
      "11 7542\n",
      "1 7620\n",
      "1 7652\n",
      "1 7670\n",
      "1 7835\n",
      "1 7839\n",
      "1 7868\n",
      "40 7920\n",
      "1 7942\n",
      "1 8012\n",
      "1 8043\n",
      "3 8112\n",
      "2 8114\n",
      "2 8128\n",
      "1 8130\n",
      "1 8135\n",
      "5 8220\n",
      "12 8273\n",
      "1 8375\n",
      "1 8455\n",
      "1 8569\n",
      "13 8584\n",
      "1 8642\n",
      "3 8671\n",
      "2 8714\n",
      "3 8827\n",
      "1 8832\n",
      "4 8870\n",
      "3 8887\n",
      "8 8968\n",
      "1 9006\n",
      "1 9049\n",
      "1 9106\n",
      "1 9227\n",
      "2 9306\n",
      "1 9372\n",
      "24 9382\n",
      "4 9412\n",
      "1 9452\n",
      "1 9581\n",
      "15 9623\n",
      "1 9646\n",
      "6 9850\n",
      "2 9851\n",
      "1 9852\n",
      "2 9937\n",
      "1 9952\n"
     ]
    }
   ],
   "source": [
    "s[0,2]\n",
    "for i in range(0, 10000):\n",
    "    if s[0, i] > 0:\n",
    "        print(s[0, i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
